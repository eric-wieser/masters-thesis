\providecommand{\thebibpath}{..}
\makeatletter\def\input@path{{\thebibpath/}{.}}\makeatother
\documentclass[main.tex]{subfiles}
\begin{document}

\section{Optimal Control of Discrete systems}
	One way to model dynamical systems is with discrete-time equations, which for a sufficiently small timestep, are close approximations to their continuous-time counterparts.
	For a suitable choice of state vector $\bm{x}$ and timestep $\delta t$, discrete-time systems can be described in terms of the transition dynamics,
	\begin{align}
		\bm{x}\tind{i + 1} = f(\bm{x}\tind{i}, \bm{u}\tind{i}) \quad \text{where} \quad \bm{x}\tind{i} := x(t_0 + i\cdot\delta t)\,. \label{eq:transition}
	\end{align}
	From this, the optimal control problem in discrete time can be stated as
	\begin{alignat}{2}
		\text{find}&& \quad
			\pi^*(\bm{x}) &= \argmin_{\pi(\bm{x})}
				J(\bm{x}\tind\cdot, \bm{u}\tind\cdot)
				\quad \text{where} \quad
				J(\bm{x}\tind\cdot, \bm{u}\tind\cdot) = \sum_{i=0}^{i=N} c(\bm{x}\tind{i}, \bm{u}\tind{i})\quad \\ \label{eq:optimal}
		\text{st.}&& \quad
			\bm{x}\tind{i+1} &= f(\bm{x}\tind{i}, \bm{u}\tind{i}) \\ \nonumber
		&&
			\bm{u}\tind{i} &= \pi(\bm{x}\tind{i})\, \nonumber
	\end{alignat}
	where $\pi(\bm{x})$ is a policy that computes the desired actions for a given state, and $c(\bm{x}, \bm{u})$ attributes a cost to a set of states and actions at a single timestep.

\section{Gaussian Processes}

	\begin{figure}[b]
		\centering
		\begin{subfigure}[t]{0.3\linewidth}
			\input{figures/gauss1d.tikz}
			\caption{1D Gaussian distribution}
			\label{fig:gaussian:1d}
		\end{subfigure}%
		\hfill
		\begin{subfigure}[t]{0.3\linewidth}
			\input{figures/gauss2d.tikz}
			\caption{2D Gaussian distribution}
			\label{fig:gaussian:2d}
		\end{subfigure}%
		\hfill
		\begin{subfigure}[t]{0.3\linewidth}
			\input{figures/gaussproc.tikz}
			\caption{Gaussian process}
			\label{fig:gaussian:proc}
		\end{subfigure}%
		\caption{Probabilistic applications of Gaussians}
		\medskip
		\small
		The grey shaded region shows a \SI{90}{\percent} confidence interval, and red markers show samples taken from the distribution. Above 1D, plotting the probability density becomes impractical.
	\end{figure}

	A Gaussian (or \enquote{normal}) distribution (\cref{fig:gaussian:1d})is described by a mean $\mu$ and a variance $\sigma^2$, and satisfies
	\begin{align}
		x &\sim \mathcal{N}(\mu, \sigma) &\implies
		p(x; \mu, \sigma^2) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left[
			-\frac{1}{2} \frac{(x - \mu)^2}{\sigma^2}
		\right]}\,.
	\intertext{
	This describes only a single variable.
	It can be extended to the multivariate Gaussian distribution over $\bm{x} \in \mathbb{R}^d$ (\cref{fig:gaussian:2d}), which is described by a mean \emph{vector} $\bm{\mu}$ and a \emph{co}variance \emph{matrix} $\Sigma$, satisfying
	}
		\bm{x} &\sim \mathcal{N}(\bm{\mu}, \Sigma) &\implies
		p(\bm{x}; \bm{\mu}, \Sigma)
			&= \frac{1}{\sqrt{(2\pi)^d|\Sigma|}} \exp{\left[
				-\frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu})
			\right]}\,.
	\end{align}
	This is able to describe correlations between the finite set of variables $x_i, i\in [1,d]$.

	Gaussian processes are a generalization of multivariate Gaussians to infinitely many variables.
	This is done by noting that the vector $\mathbf{x} \in \mathbb{R}^k$ can be re-expressed as a discrete function of the index $x\colon [1,d] \to \mathbb{R}$, where $x_i = x(i)$.
	From there, the domain can be expanded to the reals to produce a continuous scalar function, such that $x\colon \mathbb{R} \to \mathbb{R}$.
	Gaussian processes (\cref{fig:gaussian:proc}) are described by a mean \emph{function} $m(i)$ and a covariance \emph{function} $K(i, j)$, satisfying
	\begin{align}
		x(\cdot) &\sim \mathrm{GP}(m(\cdot), K(\cdot,\cdot)) \\\implies
		p(x(\cdot); m(\cdot), K(\cdot,\cdot))
			&\propto \exp{\left[
				-\frac{1}{2}
				\iint
					\left(x(i) - m(i)\right)
					{K(i,j)}^{-1}
					\left(x(j) - m(j)\right)
				\diff i \diff j
			\right]}\,.
	\end{align}

	We can take our generalization one further still, by replacing the scalar function $x\colon \mathbb{R} \to \mathbb{R}$ with a scalar field $x\colon \mathbb{R}^m \to \mathbb{R}$.
	With this modification, the only change to the above equation is that the scalars $i,j$ become the vectors $\bm{i}$, $\bm{j}$.
	By defining
	\begin{align}
		\bm{z}\tind{i} &= \begin{bmatrix}\bm{x}\tind{i} \\ \bm{u}\tind{i}\end{bmatrix} &
		\Delta\bm{x}\tind{i} &= \bm{x}_{i + 1} - \bm{x}\tind{i}\,,
	\end{align}
	we can express \cref{eq:transition} in terms of a series of convenient $\mathbb{R}^m \to \mathbb{R}$ functions $f_j$ that can be modelled by such a distribution
	\begin{align}
		\Delta\bm{x}\tind{i}_j &= f_j(\bm{z}\tind{i}) & f_j &\sim \mathrm{GP}(m_j(\bm{z}), K_j(\bm{z}_1, \bm{z}_2))\,, \label{eq:transition-gp}
	\end{align}
	giving a probabilistic representation of discrete system dynamics.
	Such a representation is able to capture model uncertainty, process noise, and observation noise.

\section{The \textsc{Pilco} approach}

	Armed with a probabilistic representation of dynamics, \citeauthor{pilco}'s \textsc{Pilco} \cite{pilco} (Probabilistic Inference for Learning Control) provides a technique to learn the parameters of this representation, and to select a probabilistically optimal controller.

	As such, the optimization in \cref{eq:optimal} also becomes probabilistic\footnotemark, with $\bm{x}\tind{i}$ and $\bm{u}\tind{i}$ become distributions over states and outputs, not simply values. These distributions can be found by sequentially applying the probabilistic one-step transition equation described in \cref{eq:transition-gp}.

	\footnotetext{
		To make the optimization well-defined, $J$ needs to be chosen to collapse the distribution back down into a scalar. A simple choice is to define $J'(...) = \mathbb{E}_{...}[J(...)]$, but more powerful options exist, such as penalizing uncertainty in the cost.
	}

	The learning process consists of the following steps, repeated cyclically until a sufficient controller is obtained:
	\begin{enumerate}[nosep]
		\item Choose an initial control policy
		\item Apply the latest policy to the robot (perform a \enquote{rollout}), recording state and action trajectories \label{list:pilco:rollout}
		\item Train the probabilistic dynamics model from all past data
		\item Choose the policy that minimizes the cost $J$ when applied to a system with the newly-learnt dynamics, and return to step \ref{list:pilco:rollout}
	\end{enumerate}
	In practice, the first time that step \ref{list:pilco:rollout} is executed, multiple rollouts will be done, in order to provide enough data to sensibly train the dynamics model.

\section{Unicycle robots}

	Unicycles provide a challenging control problem to human riders, so intuitively provide a good testing ground for new control techniques.
	A reasonable approximation for how a human rider operates a unicycle is that they have two degrees of freedom -- one through the pedals, and the other by adjusting their angular rotation through the vertical axis.
	This is mirrored in the design of the robotic unicycle, which has one motor attached to the drive wheel, and the other attached to a vertical flywheel\footnotemark (henceforth referred to as the turntable).
	The state space of such a unicycle is $\mathbb{R}^{12}$ \cite{forster}, so this system is heavily underactuated, explaining the difficulty in controlling it.

	\footnotetext{While it may seem unfair that flywheel can rotate freely, while a human is limited to about \SI{180}{\degree} of motion, this intuition doesn't tell the whole story -- human riders are able to change their moment of inertia by extending and contracting their arms.}


	There is a history of unicycle robots in the Engineering Department dating back to 2005, which is described in more detail by \citeauthor{queiro} \cite{queiro}.
	Alongside work on these, the \textsc{Pilco} method has been successfully applied to computer models of these unicycles  \cite[section~3.3]{pilco}.
	Based on the work in 2011, the decision was made to move from the large and dangerous platform described there to a much smaller and safer model.
	Since then, work by \citeauthor{aleksi} \cite{aleksi} went on to write the embedded software for the system, and performed experiments to try and reproduce the results of \textsc{Pilco} in simulation in hardware.

	This work met mixed success -- many problems in the hardware were identified and fixed, and there was some evidence of learning, with the controller marginally improving over time.
	However, various concerns were raised with the testing procedure, and the computer model of the unicycle was never updated to match the hardware -- making it difficult to judge whether problems lay in the hardware or in simply a more difficult control task than on the larger unicycle, and raising concerns about whether other parts of the software stack were still configured for the large robot.

	\subsection{Application of \textsc{Pilco} to the unicycle}
		For the purposes of \textsc{Pilco}, our action and reduced (ie., the components used for learning) state vectors are
		\begin{align}
			\bm{x} &= \begin{bmatrix}
				\dot\theta & \dot\phi &\dot\psi_w & \dot\psi & \dot\psi_t &
				x_c & y_c &
				\theta &
				\phi & \psi
			\end{bmatrix}^T &
			\bm{u} &= \begin{bmatrix}
				\tau_t & \tau_w
			\end{bmatrix}^T\,,
		\end{align}
		where $\theta$ is the roll angle, $\phi$ is yaw, $\psi_w$ wheel angle,
		$\psi$ pitch angle, $\psi_t$ the turntable angle, and $x_c, y_c$ are the position of the world-origin in the coordinate space of the robot.
		$\tau_t$ and $\tau_w$ are the control torques on the turntable and wheel, respectively.
		In simulation, some extra states ($\dot{x}_c, \dot{y}_c, \psi_w, \psi_t$) are needed in order to implement the dynamics derived by \citeauthor{forster} \cite{forster}.

		In this report, we restrict our search for optimal controllers to affine controllers of the form
		$\bm{u} = \pi(\bm{x}) = W\bm{x} + \bm{b}$, and choose cost functions of the form
		$c(\bm{x}, \bm{u}) = \mathbb{E}_{\bm{x}, \bm{u}} \exp\left[-\frac{1}{2} f(\bm{x})^T Q f(\bm{x})\right]$, where $f$ is a function that appends trigonometric functions of $\phi, \theta, \psi$ to the end of the state vector, aiding in penalization of geometric properties.

\bib

\end{document}