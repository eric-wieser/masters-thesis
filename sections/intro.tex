\providecommand{\thebibpath}{..}
\makeatletter\def\input@path{{\thebibpath/}{.}}\makeatother
\documentclass[main.tex]{subfiles}
\begin{document}

\section{Optimal Control}

	The field of control theory concerns itself with taking a system, and designing a controller that makes it behave in a desirable way.
	Optimal control lets us formally define a measure of desirability, and find the controller that maximizes that measure.

	In order to apply any control theory, we need a model of the system.
	One way to model dynamical systems is with discrete-time equations, which for a sufficiently small timestep, are close approximations to their continuous-time counterparts.
	For a suitable choice of state vector $\bm{x}$ and timestep $\delta t$, discrete-time systems can be described in terms of the transition dynamics,
	\begin{align}
		\bm{x}\tind{i + 1} = f(\bm{x}\tind{i}, \bm{u}\tind{i}) \quad \text{where} \quad \bm{x}\tind{i} := x(t_0 + i\cdot\delta t)\,. \label{eq:transition}
	\end{align}
	From this, the optimal control problem in discrete time can be stated as
	\begin{alignat}{2}
		\text{find}&& \quad
			\pi^*(\bm{x}) &= \argmin_{\pi(\bm{x})}
				J(\bm{x}\tind\cdot, \bm{u}\tind\cdot)
				\quad \text{where} \quad
				J(\bm{x}\tind\cdot, \bm{u}\tind\cdot) = \sum_{i=0}^{i=N} c(\bm{x}\tind{i}, \bm{u}\tind{i})\quad \\ \label{eq:optimal}
		\text{st.}&& \quad
			\bm{x}\tind{i+1} &= f(\bm{x}\tind{i}, \bm{u}\tind{i}) \\ \nonumber
		&&
			\bm{u}\tind{i} &= \pi(\bm{x}\tind{i})\, \nonumber
	\end{alignat}
	where $\pi(\bm{x})$ is a policy that computes the desired actions for a given state, and $c(\bm{x}, \bm{u})$ attributes a cost to a set of states and actions at a single timestep.

\section{Gaussian Processes}

	\begin{figure}[b]
		\centering
		\begin{subfigure}[t]{0.3\linewidth}
			\input{figures/gauss1d.tikz}
			\caption{1D Gaussian distribution}
			\label{fig:gaussian:1d}
		\end{subfigure}%
		\hfill
		\begin{subfigure}[t]{0.3\linewidth}
			\input{figures/gauss2d.tikz}
			\caption{2D Gaussian distribution}
			\label{fig:gaussian:2d}
		\end{subfigure}%
		\hfill
		\begin{subfigure}[t]{0.3\linewidth}
			\input{figures/gaussproc.tikz}
			\caption{Gaussian process}
			\label{fig:gaussian:proc}
		\end{subfigure}%
		\caption{Probabilistic applications of Gaussians}
		\medskip
		\small
		The grey shaded region shows a \SI{90}{\percent} confidence interval, and red markers show samples taken from the distribution. Above 1D, plotting the probability density becomes impractical.
	\end{figure}

	A Gaussian (or \enquote{normal}) distribution (\cref{fig:gaussian:1d})is described by a mean $\mu$ and a variance $\sigma^2$, and satisfies
	\begin{align}
		x &\sim \mathcal{N}(\mu, \sigma) &\implies
		p(x; \mu, \sigma^2) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left[
			-\frac{1}{2} \frac{(x - \mu)^2}{\sigma^2}
		\right]}\,.
	\intertext{
	This describes only a single variable.
	It can be extended to the multivariate Gaussian distribution over $\bm{x} \in \mathbb{R}^d$ (\cref{fig:gaussian:2d}), which is described by a mean \emph{vector} $\bm{\mu}$ and a \emph{co}variance \emph{matrix} $\Sigma$, satisfying
	}
		\bm{x} &\sim \mathcal{N}(\bm{\mu}, \Sigma) &\implies
		p(\bm{x}; \bm{\mu}, \Sigma)
			&= \frac{1}{\sqrt{(2\pi)^d|\Sigma|}} \exp{\left[
				-\frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu})
			\right]}\,.
	\end{align}
	This is able to describe correlations between the finite set of variables $x_i, i\in [1,d]$.

	Gaussian processes are a generalization of multivariate Gaussians to infinitely many variables.
	This is done by noting that the vector $\mathbf{x} \in \mathbb{R}^k$ can be re-expressed as a discrete function of the index $x\colon [1,d] \to \mathbb{R}$, where $x_i = x(i)$.
	From there, the domain can be expanded to the reals to produce a continuous scalar function, such that $x\colon \mathbb{R} \to \mathbb{R}$.
	Gaussian processes (\cref{fig:gaussian:proc}) are described by a mean \emph{function} $m(i)$ and a covariance \emph{function} $K(i, j)$, such that any finite number of samples $x(i)$ from the process are jointly gaussian. Algebraically,
	\begin{align}
		x(\cdot) &\sim \mathrm{GP}(m(\cdot), K(\cdot,\cdot)) \\
		\implies
		\begin{bmatrix}
			x(l_1) \\
			\vdots \\
			x(l_{|\mathcal{L}|})
		\end{bmatrix} &\sim \mathcal{N}\left(\begin{bmatrix}
			m(l_1) \\
			\vdots \\
			m(l_{|\mathcal{L}|})
		\end{bmatrix}, \begin{bmatrix}
			K(l_1,l_1) & \cdots & K(l_1,l_{|\mathcal{L}|}) \\
			\vdots & \ddots & \vdots \\
			x(l_{|\mathcal{L}|},l_1) & \cdots & K(l_{|\mathcal{L}|},l_{|\mathcal{L}|})
		\end{bmatrix}\right), \quad l \in \mathcal{L} \subset \mathbb{R}\,.
	\end{align}

	We can take our generalization one further still, by replacing the scalar function $x\colon \mathbb{R} \to \mathbb{R}$ with a scalar field $x\colon \mathbb{R}^m \to \mathbb{R}$.
	With this modification, the only change to the above equation is that $\bm{l} \in \mathcal{L} \subset \mathbb{R}^m$.
	By defining
	\begin{align}
		\bm{z}\tind{i} &= \begin{bmatrix}\bm{x}\tind{i} \\ \bm{u}\tind{i}\end{bmatrix} &
		\Delta\bm{x}\tind{i} &= \bm{x}\tind{i + 1} - \bm{x}\tind{i}\,,
	\end{align}
	we can express \cref{eq:transition} in terms of a series of convenient $\mathbb{R}^m \to \mathbb{R}$ functions $f_j$ that can be modelled by such a distribution
	\begin{align}
		\Delta\bm{x}\tind{i}_j &= f_j(\bm{z}\tind{i}) & f_j &\sim \mathrm{GP}(m_j(\bm{z}), K_j(\bm{z}_1, \bm{z}_2))\,, \label{eq:transition-gp}
	\end{align}
	giving a probabilistic representation of discrete system dynamics.
	Such a representation is able to capture model uncertainty, process noise, and observation noise.

\section{The \textsc{Pilco} approach}

	Armed with a probabilistic representation of dynamics, \citeauthor{pilco}'s \textsc{Pilco} \cite{pilco} (Probabilistic Inference for Learning Control) provides a technique to learn the parameters of this representation, and to select a probabilistically optimal controller.

	As such, the optimization in \cref{eq:optimal} also becomes probabilistic\footnotemark, with $\bm{x}\tind{i}$ and $\bm{u}\tind{i}$ becoming distributions over, not simply values of, states and outputs. These distributions can be found by sequentially applying the probabilistic one-step transition equation described in \cref{eq:transition-gp}.

	\footnotetext{
		To make the optimization well-defined, $J$ needs to be chosen to collapse the distribution back down into a scalar. A simple choice is to define $J'(...) = \mathbb{E}_{...}[J(...)]$, but more powerful options exist, such as penalizing uncertainty in the cost.
	}

	The learning process consists of the following steps, repeated cyclically until a sufficient controller is obtained:
	\begin{enumerate}[nosep]
		\item Choose an initial control policy
		\item Apply the latest policy to the robot (perform a \enquote{rollout}), recording state and action trajectories \label{list:pilco:rollout}
		\item Train the probabilistic dynamics model from all past data
		\item Choose the policy that minimizes the cost $J$ when applied to a system with the newly-learnt dynamics, and return to step \ref{list:pilco:rollout}
	\end{enumerate}
	In practice, the first time that step \ref{list:pilco:rollout} is executed, multiple rollouts will be done, in order to provide enough data to sensibly train the dynamics model.

	The key advantage of this method is that it requires orders of magnitude less interaction time \cite{pilco} than competing methods -â€“ important for systems like a unicycle, where each failed interaction (falling over) can cause damage.

\section{Unicycle robots}


	Unicycles provide a challenging control problem to human riders, so intuitively provide a good testing ground for new control techniques.
	A reasonable approximation for how a human rider operates a unicycle is that they have two degrees of freedom -- one through the pedals, and the other by adjusting their angular rotation through the vertical axis.
	This is mirrored in the design of the robotic unicycle, which has one motor attached to the drive wheel, and the other attached to a vertical flywheel\footnotemark (henceforth referred to as the turntable).
	The state space of such a unicycle is $\mathbb{R}^{12}$ \cite{forster}, so this system is heavily underactuated, explaining the difficulty in controlling it.

	\footnotetext{While it may seem unfair that flywheel can rotate freely, while a human is limited to about \SI{180}{\degree} of motion, this intuition doesn't tell the whole story -- human riders are able to change their moment of inertia by extending and contracting their arms.}

	\begin{figure}[!hbt]
		\hspace*{\fill}
		\begin{subfigure}{0.3\linewidth}
			\includegraphics[width=\linewidth]{figures/old.jpg}
			\caption{Old, \SI{1}{\meter} tall}
			\label{fig:uni-old}
		\end{subfigure}%
		\hfill%
		\begin{subfigure}{0.3\linewidth}
			\includegraphics[width=\linewidth]{figures/front.jpg}
			\caption{New, \SI{20}{\centi\meter} tall}
			\label{fig:uni-new}
		\end{subfigure}
		\hspace*{\fill}
		\caption{Unicycle robot configurations}
	\end{figure}

	There is a history of unicycle robots in the Engineering Department dating back to 2005, which is described in more detail by \citeauthor{queiro} \cite{queiro}.
	Alongside work on these, the \textsc{Pilco} method has been successfully applied to computer models of these unicycles  \cite[section~3.3]{pilco}.
	Based on the work in 2011, the decision was made to move from the large and dangerous platform described there (\cref{fig:uni-old}) to a much smaller and safer model (\cref{fig:uni-new}).
	Since then, work by \citeauthor{aleksi} \cite{aleksi} went on to write the embedded software for the system, and performed experiments to try and reproduce the results of \textsc{Pilco} in simulation in hardware.


	This work met mixed success -- many problems in the hardware were identified and fixed, and there was some evidence of learning, with the controller marginally improving over time.
	However, various concerns were raised with the testing procedure, and the computer model of the unicycle was never updated to match the hardware -- making it difficult to judge whether problems lay in the hardware or in simply a more difficult control task than on the larger unicycle, and raising concerns about whether other parts of the software stack were still configured for the large robot.

	\subsection{Application of \textsc{Pilco} to the unicycle}
		For the purposes of \textsc{Pilco}, our action and reduced (ie., the components used for learning) state vectors are
		\begin{align}
			\bm{x} &= \begin{bmatrix}
				\dot\theta & \dot\phi &\dot\psi_w & \dot\psi & \dot\psi_t &
				x_c & y_c &
				\theta &
				\phi & \psi
			\end{bmatrix}^T &
			\bm{u} &= \begin{bmatrix}
				\tau_t & \tau_w
			\end{bmatrix}^T\,, \label{eq:state-vars}
		\end{align}
		where $\theta$ is the roll angle, $\phi$ is yaw, $\psi_w$ wheel angle,
		$\psi$ pitch angle, $\psi_t$ the turntable angle, and $x_c, y_c$ are the position of the world-origin in the coordinate space of the robot.
		$\tau_t$ and $\tau_w$ are the control torques on the turntable and wheel, respectively.
		In simulation, some extra states ($\dot{x}_c, \dot{y}_c, \psi_w, \psi_t$) are needed in order to implement the dynamics derived by \citeauthor{forster} \cite{forster}.

		In this report, we restrict our search for optimal controllers to affine controllers of the form
		$\bm{u} = \pi(\bm{x}) = W\bm{x} + \bm{b}$, and choose cost functions of the form
		$c(\bm{x}, \bm{u}) = 1 - \mathbb{E}_{\bm{x}, \bm{u}} \exp\left[-\frac{1}{2} f(\bm{x})^T Q f(\bm{x})\right]$, where $f$ is a function that appends trigonometric functions of $\phi, \theta, \psi$ to the end of the state vector, aiding in penalization of geometric properties.

		Each GP modelling the dynamics of a single state (\cref{eq:transition-gp}) is parametrized by $m$, $K$ of the form
		\begin{align}
			m(\bm{z}) &= \bm{z} \cdot \bm{w} + b \\
			K(\bm{z}_1, \bm{z}_2) &= \sigma_s^2 \exp \left(
				-\frac{1}{2}
				\bm{z}_1^T
				\begin{bmatrix}
					\bm{l}_1^2 && \\
					& \ddots & \\
					&& \bm{l}_n^2
				\end{bmatrix}^{-1}
				\bm{z}_2
			\right) + \sigma_n^2 I \,,
		\end{align}
		where $\sigma_s^2$ and $\sigma_n^2$ estimate the signal and noise variances, $\bm{w}$ (weight) and $b$ (bias) define a simple affine mean function, and $\bm{l}$ (length) defines a length-scale for the problem for each of the state variables.
		Note that these parameters have a unique value for each state.

\section{Goals and direction of this work}
	\label{sec:dry}

	Like previous work, the ideal goal of this project is to apply \textsc{Pilco} to the unicycle robot, and achieve an outcome of a successfully balancing controller.
	Previous work went to great lengths experimentally to achieve this goal, with limited success. Thankfully, this work was not wasted, as it raised and fixed mechanical and electrical problems, freeing this work from their hindrances.

	However, few problems were traced back as far as the \textsc{Pilco} software -- indeed, none of the changes made to it during \cite{aleksi} were kept, indicating that they were at best hacks.
	One of the problems with machine learning is that it's very hard to tell the difference between failure to learn due to a hard problem or insufficiently flexible model, and due to programmatic errors in the implementation.
	This work therefore took on an approach of critically reviewing the source code used, targeting problems with non-obvious repercussions.

	In the process of reviewing source code, software engineering best-practices were kept in mind, and applied retrospectively to correct and incorrect code alike.
	A particularly important principle in software design is DRY, or \enquote{Don't Repeat Yourself}.
	More specifically, this advocates that \enquote{Every piece of knowledge must have a single, unambiguous, authoritative representation within a system}\cite{dry}.
	To see the value in holding this principle, consider a case where there are two representations. If the piece of knowledge changes, such as the mass of the robot decreasing, now two places need updating. If one of them is missed, then the result is an incorrect system. This principle applies not only to data, but also procedures.





\bib

\end{document}